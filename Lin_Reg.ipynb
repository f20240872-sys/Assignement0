{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "LvkxJLK872xG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uJEzeBaYrp3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylSYM7srAaTK"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8TzpYrAaTK"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko0DUQMIAaTL"
      },
      "source": [
        "### **Exploring the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiIvXaDLAaTL"
      },
      "source": [
        "Let's start with loading the training data from the csv into a pandas dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Load the datasets from GitHub. Train dataset has already been loaded for you in df below. To get test dataset use the commented code."
      ],
      "metadata": {
        "id": "CeVVY-uHrsqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dj_ZCCgTAaTL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/train_processed_splitted.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-4Ujl1AaTL"
      },
      "source": [
        "Let's see what the first 5 rows of this dataset looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ieOW_eQAaTL"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNnfJbohAaTM"
      },
      "source": [
        "What are all the features present? What is the range for each of the features along with their mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0e6ZqO6AaTM"
      },
      "outputs": [],
      "source": [
        "# List of all columns (features)\n",
        "print(\"Features present:\\n\", df.columns.tolist())\n",
        "\n",
        "# Compute summary statistics for all features\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hjdd5Y5AaTM"
      },
      "source": [
        "### **Feature Scaling and One-Hot Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3ls8u3hAaTM"
      },
      "source": [
        "You must have noticed that some features `(such as Utilities)` are not continuous values.\n",
        "  \n",
        "These features contain values indicating different categories and must somehow be converted to numbers so that the computer can understand it. `(Computers only understand numbers and not strings)`\n",
        "  \n",
        "These features are called categorical features. We can represent these features as a `One-Hot Representation`\n",
        "  \n",
        "  \n",
        "You must have also noticed that all the other features, each are in a different scale. This can be detremental to the performance of our linear regression model and so we normalize them so that all of them are in the range $[0,1]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeoczqQIAaTM"
      },
      "source": [
        "> NOTE: When you are doing feature scaling, store the min/max which you will use to normalize somewhere. This is then to be used at testing time. Try to think why are doing this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W17LZP2qAaTN"
      },
      "outputs": [],
      "source": [
        "# Do the one-hot encoding here\n",
        "# This finds the 'Utilities' column and replaces it with new columns for each category\n",
        "# e.g., 'Utilities_AllPub', 'Utilities_NoSeWa'\n",
        "# We set drop_first=True to avoid multicollinearity, which is important for linear models\n",
        "df_processed = pd.get_dummies(df_processed, columns=['Utilities'], drop_first=True)\n",
        "\n",
        "# You can now see the new numerical columns instead of 'Utilities'\n",
        "print(\"Data after One-Hot Encoding:\")\n",
        "print(df_processed.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG7rV9TCAaTN"
      },
      "outputs": [],
      "source": [
        "# Do the feature scaling here\n",
        "# 1. First, identify all numerical columns. We'll exclude IDs and the target variable 'SalePrice'\n",
        "# as you don't want to scale them.\n",
        "numerical_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
        "cols_to_scale = numerical_cols.drop(['Id', 'SalePrice']) # Don't scale the ID or the target\n",
        "\n",
        "# 2. Initialize the scaler\n",
        "# This object is what you will \"store\"\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# 3. Fit and Transform the training data\n",
        "# .fit() calculates the min and max for each column\n",
        "# .transform() applies the (X - min) / (max - min) formula\n",
        "df_processed[cols_to_scale] = scaler.fit_transform(df_processed[cols_to_scale])\n",
        "\n",
        "# Now all your numerical features are between 0 and 1\n",
        "print(\"\\nData after Feature Scaling (showing scaled columns):\")\n",
        "print(df_processed[cols_to_scale].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcZxb-V4AaTN"
      },
      "source": [
        "### **Conversion to NumPy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-_3l_1iAaTN"
      },
      "source": [
        "Ok so now that we have all preprocessed all the data, we need to convert it to numpy for our linear regression model\n",
        "  \n",
        "Assume that our dataset has a total of $N$ datapoints. Each datapoint having a total of $D$ features (after one-hot encoding), we want our numpy array to be of shape $(N, D)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJmGKlEgAaTN"
      },
      "source": [
        "In our task, we have to predict the `SalePrice`. We will need 2 numpy arrays $\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "(X, Y)$. These represent the features and targets respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJk8M5QmAaTN"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy array\n",
        "import numpy as np\n",
        "\n",
        "# 1. Create the Y (target) numpy array\n",
        "# We select the 'SalePrice' column and use .values to get its numpy representation\n",
        "Y = df_processed['SalePrice'].values\n",
        "\n",
        "# 2. Create the X (features) numpy array\n",
        "# We drop the target ('SalePrice') and the 'Id' column, then get the numpy representation\n",
        "X = df_processed.drop(columns=['SalePrice', 'Id']).values\n",
        "\n",
        "# 3. Check the shapes (N = datapoints, D = features)\n",
        "N = X.shape[0]\n",
        "D = X.shape[1]\n",
        "\n",
        "print(f\"Shape of X (features): {X.shape}\")\n",
        "print(f\"Shape of Y (target): {Y.shape}\")\n",
        "print(f\"Total datapoints (N): {N}\")\n",
        "print(f\"Total features (D): {D}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7YT1X-3AaTN"
      },
      "source": [
        "## Linear Regression formulation\n",
        "  \n",
        "We now have our data in the form we need. Let's try to create a linear model to get our initial (Really bad) prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgXFnXgAaTN"
      },
      "source": [
        "Let's say a single datapoint in our dataset consists of 3 features $(x_1, x_2, x_3)$, we can pose it as a linear equation as follows:\n",
        "$$ y = w_1x_1 + w_2x_2 + w_3x_3 + b $$\n",
        "Here we have to learn 4 parameters $(w_1, w_2, w_3, b)$\n",
        "  \n",
        "  \n",
        "Now how do we extend this to multiple datapoints?  \n",
        "  \n",
        "  \n",
        "Try to answer the following:\n",
        "- How many parameters will we have to learn in the cae of our dataset? (Don't forget the bias term)\n",
        "- Form a linear equation for our dataset. We need just a single matrix equation which correctly represents all the datapoints in our dataset\n",
        "- Implement the linear equation as an equation using NumPy arrays (Start by randomly initializing the weights from a standard normal distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvB5DGT7AaTO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# We assume X and Y are already defined from the previous step.\n",
        "# For this example, let's create placeholders if they don't exist.\n",
        "# (You can skip this part if you are running this in the same notebook as the previous step)\n",
        "try:\n",
        "    X.shape\n",
        "    Y.shape\n",
        "except NameError:\n",
        "    print(\"X and Y not found, creating dummy data for demonstration.\")\n",
        "    N_dummy, D_dummy = 100, 10 # 100 datapoints, 10 features\n",
        "    X = np.random.rand(N_dummy, D_dummy)\n",
        "    Y = np.random.rand(N_dummy)\n",
        "\n",
        "# --- Start of the required implementation ---\n",
        "\n",
        "# Set a random seed for reproducibility (so we all get the same \"random\" weights)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Get the dimensions of our data\n",
        "N, D = X.shape\n",
        "\n",
        "# 1. Initialize the parameters\n",
        "# W = weights, initialized from a standard normal distribution\n",
        "# The shape must be (D, 1)\n",
        "W = np.random.randn(D, 1)\n",
        "\n",
        "# b = bias, initialized to zero (a common practice)\n",
        "b = np.zeros(1)\n",
        "\n",
        "# 2. Implement the linear equation to get our (bad) initial predictions\n",
        "# We use the '@' operator for matrix multiplication (np.dot(X, W) also works)\n",
        "Y_pred = (X @ W) + b\n",
        "\n",
        "# 3. Check the results\n",
        "print(f\"Shape of X: {X.shape} (N, D)\")\n",
        "print(f\"Shape of W: {W.shape} (D, 1)\")\n",
        "print(f\"Shape of b: {b.shape} (a scalar)\")\n",
        "print(f\"---\")\n",
        "print(f\"Shape of Y_pred: {Y_pred.shape} (N, 1)\")\n",
        "print(f\"\\nFirst 5 predictions:\\n{Y_pred[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ldc0Ap_AaTO"
      },
      "source": [
        "How well does our model perform? Try comparing our predictions with the actual values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnnX7hG5AaTO"
      },
      "outputs": [],
      "source": [
        "N = Y.shape[0]\n",
        "\n",
        "# 1. Calculate the errors (residuals)\n",
        "# Y is (N,) and Y_pred is (N, 1). We need to reshape Y to (N, 1) to subtract them.\n",
        "# .reshape(-1, 1) tells numpy to make it a column vector with N rows.\n",
        "errors = Y.reshape(-1, 1) - Y_pred\n",
        "\n",
        "# 2. Square the errors\n",
        "squared_errors = errors**2\n",
        "\n",
        "# 3. Calculate the mean (the average)\n",
        "mse = np.mean(squared_errors)\n",
        "\n",
        "print(f\"Our initial, random model's Mean Squared Error (MSE) is: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MryM_jtQAaTO"
      },
      "source": [
        "### **Learning weights using gradient descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq9rq3T9AaTO"
      },
      "source": [
        "So these results are really horrible. We need to somehow update our weights so that it correclty represents our data. How do we do that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EKd0mhkAaTO"
      },
      "source": [
        "We must do the following:\n",
        "- We need some numerical indication for our performance, for this we define a Loss Function ( $\\mathscr{L}$ )\n",
        "- Find the gradients of the `Loss` with respect to the `Weights`\n",
        "- Update the weights in accordance to the gradients: $W = W - \\alpha\\nabla_W \\mathscr{L}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUfs8wh3AaTO"
      },
      "source": [
        "Lets define the loss function:\n",
        "- We will use the MSE loss since it is a regression task. (Specify the assumptions we make while doing so as taught in the class).\n",
        "- Implement this loss as a function. (Use numpy as much as possible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4G28aKRAaTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "52dc1e19-53e4-4c16-d276-c1e300c7fd08"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-957558406.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-957558406.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def mse_loss_fn(y_true, y_pred):\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "def mse_loss_fn(y_true, y_pred):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnb9WpfAaTP"
      },
      "source": [
        "Calculate the gradients of the loss with respect to the weights (and biases). First write the equations down on a piece of paper, then proceed to implement it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDYpZjjMAaTP"
      },
      "outputs": [],
      "source": [
        "def get_gradients(y_true, y_pred, W, b, X):\n",
        "    \"\"\"\n",
        "    Calculates the gradients for the MSE loss function with respect to the weights (and bias)\n",
        "\n",
        "    Args:\n",
        "        y_true: The true values of the target variable (SalePrice in our case)\n",
        "        y_pred: The predicted values of the target variable using our model (W*X + b)\n",
        "\n",
        "        W: The weights of the model\n",
        "        b: The bias of the model\n",
        "        X: The input features\n",
        "\n",
        "    Returns:\n",
        "        dW: The gradients of the loss function with respect to the weights\n",
        "        db: The gradients of the loss function with respect to the bias\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfbjlNBXAaTP"
      },
      "source": [
        "Update the weights using the gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xstl9DpAaTP"
      },
      "outputs": [],
      "source": [
        "def update(weights, bias, gradients_weights, gradients_bias, lr):\n",
        "    \"\"\"\n",
        "    Updates the weights (and bias) using the gradients and the learning rate\n",
        "\n",
        "    Args:\n",
        "        weights: The current weights of the model\n",
        "        bias: The current bias of the model\n",
        "\n",
        "        gradients_weights: The gradients of the loss function with respect to the weights\n",
        "        gradients_bias: The gradients of the loss function with respect to the bias\n",
        "\n",
        "        lr: The learning rate\n",
        "\n",
        "    Returns:\n",
        "        weights_new: The updated weights of the model\n",
        "\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZqXDTjFAaTP"
      },
      "source": [
        "Put all these together to find the loss value, its gradient and finally updating the weights in a loop. Feel free to play around with different learning rates and epochs\n",
        "  \n",
        "> NOTE: The code in comments are just meant to be used as a guide. You will have to do changes based on your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3BVX4nlAaTP"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 1000\n",
        "LEARNING_RATE = 2e-2\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    y_pred = x @ w.T + b\n",
        "    loss = mse_loss_fn(y, y_pred)\n",
        "    losses.append(loss)\n",
        "    dw, db = get_gradients(y, y_pred, w, b, x)\n",
        "    w, b = update(w, b, dw, db, LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYgND-OLAaTQ"
      },
      "source": [
        "Now use matplotlib to plot the loss graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxFqG5GSAaTQ"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaZdAzbvAaTQ"
      },
      "source": [
        "### **Testing with test data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSbgtM6AaTQ"
      },
      "source": [
        "Load and apply all the preprocessing steps used in the training data for the testing data as well. Remember to use the **SAME** min/max values which you used for the training set and not recalculate them from the test set. Also mention why we are doing this.\n",
        "\n",
        "To load test data from GitHub, use the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iuQ4ulhAaTQ"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('https://raw.githubusercontent.com/cronan03/DevSoc_AI-ML/main/test_processed_splitted.csv')\n",
        "print(df_test)\n",
        "\n",
        "# Let's find all the columns that are missing in the test set\n",
        "missing_cols = set(df.columns) - set(df_test.columns)\n",
        "\n",
        "# Add these columns to the test set with all zeros\n",
        "for col in missing_cols:\n",
        "    df_test[col] = 0\n",
        "\n",
        "if 'Utilities_AllPub' not in df_test.columns:\n",
        "    df_test = df_test.join(pd.get_dummies(df_test['Utilities'], dtype = 'int32', prefix = 'Utilities'))\n",
        "    df_test = df_test.drop('Utilities', axis = 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxqOWigRAaTQ"
      },
      "source": [
        "Using the weights learnt above, predict the values in the test dataset. Also answer the following questions:\n",
        "- Are the predictions good?\n",
        "- What is the MSE loss for the testset\n",
        "- Is the MSE loss for testing greater or lower than training\n",
        "- Why is this the case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvF1EJfZAaTQ"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "\n",
        "# Fill NaN values\n",
        "df_test.fillna(0, inplace=True)\n",
        "\n",
        "# Scale features\n",
        "\n",
        "\n",
        "# Check for unexpected NaNs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert to numpy array\n",
        "x_test = df_test.copy().drop('SalePrice', axis=1).to_numpy() # (N, D)\n",
        "y_test = df_test.copy()['SalePrice'].to_numpy().reshape(-1, 1) # (N, 1)\n",
        "print(x_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extra_cols = list(set(df_test.columns) - set(df.columns))\n",
        "print(\"Extra columns in df_test:\", extra_cols)\n",
        "\n",
        "missing_cols = list(set(df.columns) - set(df_test.columns))\n",
        "print(\"Missing columns in df_test:\", missing_cols)"
      ],
      "metadata": {
        "id": "MkmpTFp8yi1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred_test = x_test @ w.T + b # (N, 1)\n",
        "loss_test = mse_loss_fn(y_pred_test, y_test)\n",
        "\n",
        "\n",
        "# Scale the predictions back to the original scale\n"
      ],
      "metadata": {
        "id": "Z-TbJp0ntTip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(0, x_test.shape[0], 5)\n",
        "y_pred_test_sample = y_pred_test_scaled[idx].round().astype(int)\n",
        "y_true_test_sample = y_test_scaled[idx].round().astype(int)\n",
        "\n",
        "print('Predicted SalePrice: \\t', y_pred_test_sample.squeeze().tolist())\n",
        "print('Actual SalePrice: \\t', y_true_test_sample.squeeze().tolist())\n",
        "print('\\nTest Loss: \\t\\t', loss_test)"
      ],
      "metadata": {
        "id": "GIGSX11u62JU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}